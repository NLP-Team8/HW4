{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Vn6_idrp5GB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k9Yjxl17p5GC"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"preprocessed_data/train.csv\")\n",
        "df_test = pd.read_csv(\"preprocessed_data/test.csv\")\n",
        "df_val = pd.read_csv(\"preprocessed_data/val.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "No5eKD5Np5GC"
      },
      "outputs": [],
      "source": [
        "def prepare_data(df):\n",
        "    df['text'] = df['transcription'] + \" [SEP] \" + df['description']\n",
        "    return df['text'].tolist()\n",
        "\n",
        "train_texts = prepare_data(df_train)\n",
        "val_texts = prepare_data(df_val)\n",
        "test_texts = prepare_data(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjAD1pHxp5GD",
        "outputId": "8097d7cc-222e-45f1-96c2-ef09e1cd9287"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/aabavandpour/anaconda3/envs/Alireza/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pad token set to: <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Verify that the pad token is set correctly\n",
        "print(\"Pad token set to:\", tokenizer.pad_token)\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=\"longest\", max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=\"longest\", max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z7HrUAFLp5GD"
      },
      "outputs": [],
      "source": [
        "class Medical_dataset(Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return input_ids as labels for model training\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = item['input_ids'].clone()\n",
        "        return item\n",
        "\n",
        "train_dataset = Medical_dataset(train_encodings)\n",
        "val_dataset = Medical_dataset(val_encodings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMcTUKX6p5GD",
        "outputId": "fd031e44-85f1-40b7-9d9b-601223af0d1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('FinancialSupport/gpt2-ft-medical-qa')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "O9AAAASIq17Z",
        "outputId": "24fa3ef3-4ee7-48eb-f270-76827760c611"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                       transcription  \\\n",
            "0  history of present illness:  ,the patient is a...   \n",
            "1  hx: ,this 46y/o rhm with htn was well until 2 ...   \n",
            "2  title of operation: , placement of right new v...   \n",
            "\n",
            "                                         description  \\\n",
            "0   a woman presenting to our clinic for the firs...   \n",
            "1   patient with sudden onset dizziness and rue c...   \n",
            "2   placement of right new ventriculoperitoneal (...   \n",
            "\n",
            "                               generated_description  bleu_score  \n",
            "0  history of present illness: ,the patient is a ...    0.045940  \n",
            "1  hx:,this 46y/o rhm with htn was well until 2 w...    0.006950  \n",
            "2  title of operation:, placement of right new ve...    0.019549  \n",
            "Average BLEU score on the first 3 rows of the validation set: 0.024146234789758\n"
          ]
        }
      ],
      "source": [
        "def generate_description(transcription):\n",
        "    input_text = transcription\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "    # Check if input_ids exceed the model's vocab size\n",
        "    if torch.max(input_ids) >= tokenizer.vocab_size:\n",
        "        raise ValueError(\"Input IDs contain indices outside the model's vocabulary size.\")\n",
        "    \n",
        "    # Add attention mask creation\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(device)\n",
        "\n",
        "    # Adjust max_length if necessary\n",
        "    max_length = min(4096, model.config.n_positions)\n",
        "\n",
        "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_return_sequences=1)\n",
        "    description = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    description = description.split(\"[SEP]\")[-1].strip()\n",
        "    return description\n",
        "\n",
        "# Select the first 3 rows of the validation set\n",
        "df_val_sample = df_val.head(3).copy()\n",
        "\n",
        "# Generate descriptions for the first 3 rows of the validation set\n",
        "df_val_sample['generated_description'] = df_val_sample['transcription'].apply(generate_description)\n",
        "\n",
        "# Calculate BLEU score\n",
        "def calculate_bleu(reference, candidate):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
        "\n",
        "df_val_sample['bleu_score'] = df_val_sample.apply(lambda row: calculate_bleu(row['description'], row['generated_description']), axis=1)\n",
        "\n",
        "# Print the BLEU scores for the first 3 rows\n",
        "print(df_val_sample[['transcription', 'description', 'generated_description', 'bleu_score']])\n",
        "\n",
        "# Print the average BLEU score for the first 3 rows\n",
        "average_bleu_score = df_val_sample['bleu_score'].mean()\n",
        "print(f\"Average BLEU score on the first 3 rows of the validation set: {average_bleu_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Qvh6cehg4sCA"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50260, 1280)\n",
              "    (wpe): Embedding(1024, 1280)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-35): 36 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1280, out_features=50260, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/aabavandpour/anaconda3/envs/Alireza/lib/python3.8/site-packages/accelerate/accelerator.py:451: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # number of training epochs\n",
        "    per_device_train_batch_size=2,   # batch size for training\n",
        "    per_device_eval_batch_size=8,    # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    evaluation_strategy=\"epoch\",     # evaluate each epoch\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/aabavandpour/anaconda3/envs/Alireza/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1491' max='1491' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1491/1491 15:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.134238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.771300</td>\n",
              "      <td>1.954901</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.129300</td>\n",
              "      <td>1.912344</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/aabavandpour/anaconda3/envs/Alireza/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
            "/local/aabavandpour/anaconda3/envs/Alireza/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1491, training_loss=2.3001555066073522, metrics={'train_runtime': 909.5495, 'train_samples_per_second': 13.101, 'train_steps_per_second': 1.639, 'total_flos': 3113555853312000.0, 'train_loss': 2.3001555066073522, 'epoch': 3.0})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                       transcription  \\\n",
            "0  history of present illness:  ,the patient is a...   \n",
            "1  hx: ,this 46y/o rhm with htn was well until 2 ...   \n",
            "2  title of operation: , placement of right new v...   \n",
            "\n",
            "                                         description  \\\n",
            "0   a woman presenting to our clinic for the firs...   \n",
            "1   patient with sudden onset dizziness and rue c...   \n",
            "2   placement of right new ventriculoperitoneal (...   \n",
            "\n",
            "                               generated_description  bleu_score  \n",
            "0  history of present illness: ,the patient is a ...    0.044965  \n",
            "1  hx:,this 46y/o rhm with htn was well until 2 w...    0.006279  \n",
            "2  placement of right new ventriculoperitoneal (v...    1.000000  \n",
            "Average BLEU score on the first 3 rows of the validation set: 0.3504146379791368\n"
          ]
        }
      ],
      "source": [
        "def generate_description(transcription):\n",
        "    input_text = transcription\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "\n",
        "    # Check if input_ids exceed the model's vocab size\n",
        "    if torch.max(input_ids) >= tokenizer.vocab_size:\n",
        "        raise ValueError(\"Input IDs contain indices outside the model's vocabulary size.\")\n",
        "    \n",
        "    # Add attention mask creation\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(device)\n",
        "\n",
        "    # Adjust max_length if necessary\n",
        "    max_length = min(4096, model.config.n_positions)\n",
        "\n",
        "    output = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, num_return_sequences=1)\n",
        "    description = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    description = description.split(\"[SEP]\")[-1].strip()\n",
        "    return description\n",
        "\n",
        "# Select the first 3 rows of the validation set\n",
        "df_val_sample = df_val.head(3).copy()\n",
        "\n",
        "# Generate descriptions for the first 3 rows of the validation set\n",
        "df_val_sample['generated_description'] = df_val_sample['transcription'].apply(generate_description)\n",
        "\n",
        "# Calculate BLEU score\n",
        "def calculate_bleu(reference, candidate):\n",
        "    reference = [reference.split()]\n",
        "    candidate = candidate.split()\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=smoothing_function)\n",
        "\n",
        "df_val_sample['bleu_score'] = df_val_sample.apply(lambda row: calculate_bleu(row['description'], row['generated_description']), axis=1)\n",
        "\n",
        "# Print the BLEU scores for the first 3 rows\n",
        "print(df_val_sample[['transcription', 'description', 'generated_description', 'bleu_score']])\n",
        "\n",
        "# Print the average BLEU score for the first 3 rows\n",
        "average_bleu_score = df_val_sample['bleu_score'].mean()\n",
        "print(f\"Average BLEU score on the first 3 rows of the validation set: {average_bleu_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
