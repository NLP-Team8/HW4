{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Falconsai/medical_summarization Model\n",
    "\n",
    "This notebook demonstrates how to fine-tune the `Falconsai/medical_summarization` model on a medical dataset. The dataset consists of transcriptions and descriptions, and we will use both ROUGE and BLEU metrics to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "from datasets import load_metric, Dataset, DatasetDict\n",
    "import sacrebleu\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv('preprocessed_data/train.csv')\n",
    "val_df = pd.read_csv('preprocessed_data/val.csv')\n",
    "test_df = pd.read_csv('preprocessed_data/test.csv')\n",
    "\n",
    "# Extract transcriptions and descriptions\n",
    "train_transcriptions = train_df['transcription'].tolist()\n",
    "train_descriptions = train_df['description'].tolist()\n",
    "val_transcriptions = val_df['transcription'].tolist()\n",
    "val_descriptions = val_df['description'].tolist()\n",
    "test_transcriptions = test_df['transcription'].tolist()\n",
    "test_descriptions = test_df['description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('Falconsai/medical_summarization')\n",
    "\n",
    "# Tokenize the datasets\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['transcription'], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    outputs = tokenizer(examples['description'], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs['labels'] = outputs['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Create Hugging Face datasets\n",
    "train_data = Dataset.from_dict({\n",
    "    'transcription': train_transcriptions,\n",
    "    'description': train_descriptions\n",
    "})\n",
    "val_data = Dataset.from_dict({\n",
    "    'transcription': val_transcriptions,\n",
    "    'description': val_descriptions\n",
    "})\n",
    "test_data = Dataset.from_dict({\n",
    "    'transcription': test_transcriptions,\n",
    "    'description': test_descriptions\n",
    "})\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data.map(preprocess_function, batched=True),\n",
    "    'validation': val_data.map(preprocess_function, batched=True),\n",
    "    'test': test_data.map(preprocess_function, batched=True)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('Falconsai/medical_summarization')\n",
    "model.to(device)\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "rouge_metric = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ROUGE scores\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    rouge_result = {key: value.mid.fmeasure * 100 for key, value in rouge_result.items()}\n",
    "    \n",
    "    # BLEU scores\n",
    "    bleu = sacrebleu.corpus_bleu(decoded_preds, [decoded_labels])\n",
    "    bleu_result = {\"bleu\": bleu.score}\n",
    "    \n",
    "    # Combine both results\n",
    "    result = {**rouge_result, **bleu_result}\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate the model on test data\n",
    "results = trainer.evaluate(eval_dataset=dataset['test'])\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
